---
title: Create a Tool
description: Build a test runner tool that agents can use
---

import { Accordions, Accordion } from 'fumadocs-ui/components/accordion';
import { Cards, Card } from 'fumadocs-ui/components/card';
import { Callout } from 'fumadocs-ui/components/callout';

## What You'll Build

In this guide, you'll create a **Test Runner Tool** — an external integration that allows agents to execute test suites and get results.

<Callout type="info">
**Time:** ~15 minutes
**Prerequisites:** agentic-p installed, repository initialized, Python 3.11+
</Callout>

## What Are Tools?

Tools are **external system integrations** that give agents capabilities beyond text generation:

- **APIs**: REST/GraphQL endpoints
- **Shell**: Command execution
- **Files**: Read/write operations
- **Databases**: Query execution

## Step 1: Create the Tool

```bash
agentic-p new tool shell/run-tests -d "Execute test suite and return results"
```

This creates:
```
primitives/v1/tools/shell/run-tests/
├── run-tests.tool.yaml
├── impl.local.py
└── README.md
```

## Step 2: Define the Tool Specification

Edit `run-tests.tool.yaml`:

```yaml
spec_version: "v1"
type: tool
name: run-tests
description: "Execute test suite and return results"

# Input schema (what the agent provides)
input_schema:
  type: object
  properties:
    path:
      type: string
      description: "Path to test file or directory"
    pattern:
      type: string
      description: "Test name pattern to match (e.g., 'test_auth*')"
    verbose:
      type: boolean
      description: "Show detailed output"
      default: false
    coverage:
      type: boolean
      description: "Run with coverage reporting"
      default: false
  required:
    - path

# Output schema (what the tool returns)
output_schema:
  type: object
  properties:
    success:
      type: boolean
      description: "Whether all tests passed"
    total:
      type: integer
      description: "Total number of tests"
    passed:
      type: integer
      description: "Number of passed tests"
    failed:
      type: integer
      description: "Number of failed tests"
    skipped:
      type: integer
      description: "Number of skipped tests"
    duration:
      type: number
      description: "Test duration in seconds"
    output:
      type: string
      description: "Test output (truncated if too long)"
    failures:
      type: array
      description: "Details of failed tests"
      items:
        type: object
        properties:
          name:
            type: string
          error:
            type: string

# Available implementations
implementations:
  - provider: local
    runtime: python
    entrypoint: impl.local.py
```

## Step 3: Implement the Tool

Edit `impl.local.py`:

```python
#!/usr/bin/env python3
"""Test runner tool implementation."""

import json
import subprocess
import sys
from pathlib import Path


def run_tests(
    path: str,
    pattern: str | None = None,
    verbose: bool = False,
    coverage: bool = False,
) -> dict:
    """
    Execute pytest and return results.

    Args:
        path: Path to test file or directory
        pattern: Test name pattern to match
        verbose: Show detailed output
        coverage: Run with coverage

    Returns:
        Test results dictionary
    """
    # Build command
    cmd = ["python", "-m", "pytest", path, "--tb=short", "-q"]

    if pattern:
        cmd.extend(["-k", pattern])

    if verbose:
        cmd.append("-v")

    if coverage:
        cmd.extend(["--cov", "--cov-report=term-missing"])

    # Add JSON output for parsing
    cmd.extend(["--json-report", "--json-report-file=-"])

    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=300,  # 5 minute timeout
            cwd=Path(path).parent if Path(path).is_file() else path,
        )

        # Try to parse JSON output
        try:
            # pytest-json-report outputs JSON
            report = json.loads(result.stdout)
            return format_report(report)
        except json.JSONDecodeError:
            # Fall back to parsing text output
            return parse_text_output(result)

    except subprocess.TimeoutExpired:
        return {
            "success": False,
            "error": "Test execution timed out after 5 minutes",
            "total": 0,
            "passed": 0,
            "failed": 0,
            "skipped": 0,
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "total": 0,
            "passed": 0,
            "failed": 0,
            "skipped": 0,
        }


def format_report(report: dict) -> dict:
    """Format pytest-json-report output."""
    summary = report.get("summary", {})

    failures = []
    for test in report.get("tests", []):
        if test.get("outcome") == "failed":
            failures.append({
                "name": test.get("nodeid", "unknown"),
                "error": test.get("call", {}).get("longrepr", "Unknown error"),
            })

    return {
        "success": summary.get("failed", 0) == 0,
        "total": summary.get("total", 0),
        "passed": summary.get("passed", 0),
        "failed": summary.get("failed", 0),
        "skipped": summary.get("skipped", 0),
        "duration": report.get("duration", 0),
        "failures": failures,
    }


def parse_text_output(result: subprocess.CompletedProcess) -> dict:
    """Parse pytest text output as fallback."""
    output = result.stdout + result.stderr

    # Simple parsing of summary line
    # e.g., "5 passed, 2 failed in 1.23s"
    passed = failed = skipped = 0

    for line in output.split("\n"):
        if "passed" in line or "failed" in line:
            import re
            if m := re.search(r"(\d+) passed", line):
                passed = int(m.group(1))
            if m := re.search(r"(\d+) failed", line):
                failed = int(m.group(1))
            if m := re.search(r"(\d+) skipped", line):
                skipped = int(m.group(1))

    return {
        "success": result.returncode == 0,
        "total": passed + failed + skipped,
        "passed": passed,
        "failed": failed,
        "skipped": skipped,
        "output": output[:2000],  # Truncate long output
    }


if __name__ == "__main__":
    # Read input from stdin (JSON)
    input_data = json.loads(sys.stdin.read())

    # Run tests
    result = run_tests(
        path=input_data["path"],
        pattern=input_data.get("pattern"),
        verbose=input_data.get("verbose", False),
        coverage=input_data.get("coverage", False),
    )

    # Output result as JSON
    print(json.dumps(result))
```

## Step 4: Add Provider Bindings (Optional)

For Claude MCP integration, create `impl.claude.yaml`:

```yaml
provider: claude
sdk_version: "1.0"

mcp_server:
  name: "test-runner"
  transport: stdio
  command: "uv"
  args: ["run", "python", "impl.local.py"]

# Environment variables the tool needs
env:
  - PYTHONPATH
  - PATH
```

## Step 5: Test the Tool Locally

```bash
# Test with sample input
echo '{"path": "tests/", "verbose": true}' | python primitives/v1/tools/shell/run-tests/impl.local.py
```

## Step 6: Validate and Build

```bash
# Validate
agentic-p validate

# Build
agentic-p build --provider claude

# Install
agentic-p install --provider claude --project
```

## Step 7: Use the Tool

Agents can now use your tool:

```
Please run the tests in the tests/unit/ directory and tell me which ones failed.
```

The agent will invoke the `run-tests` tool and interpret the results.

## Adding Tests

Create tests for your tool in `tests/`:

```python
# tests/test_run_tests_tool.py
import json
import subprocess
from pathlib import Path

TOOL_PATH = Path("primitives/v1/tools/shell/run-tests/impl.local.py")


def test_run_tests_success():
    """Test successful test run."""
    input_data = json.dumps({"path": "tests/fixtures/passing/"})

    result = subprocess.run(
        ["python", str(TOOL_PATH)],
        input=input_data,
        capture_output=True,
        text=True,
    )

    output = json.loads(result.stdout)
    assert output["success"] is True
    assert output["passed"] > 0
    assert output["failed"] == 0


def test_run_tests_with_failures():
    """Test run with failing tests."""
    input_data = json.dumps({"path": "tests/fixtures/failing/"})

    result = subprocess.run(
        ["python", str(TOOL_PATH)],
        input=input_data,
        capture_output=True,
        text=True,
    )

    output = json.loads(result.stdout)
    assert output["success"] is False
    assert output["failed"] > 0
    assert len(output["failures"]) > 0
```

## Best Practices

<Accordions>
  <Accordion title="Define clear schemas">
    Input and output schemas help agents understand how to use the tool and interpret results.
  </Accordion>
  <Accordion title="Handle errors gracefully">
    Always return structured error information. Never crash or hang.
  </Accordion>
  <Accordion title="Set timeouts">
    Prevent runaway processes with appropriate timeouts.
  </Accordion>
  <Accordion title="Limit output size">
    Truncate large outputs to prevent context overflow.
  </Accordion>
</Accordions>

## Next Steps

<Cards>
  <Card
    title="Create a Hook"
    href="/docs/guides/create-hook"
  >
    Add safety and observability
  </Card>
  <Card
    title="Deploy to Claude"
    href="/docs/guides/deploy-claude"
  >
    Deploy your primitives
  </Card>
</Cards>
